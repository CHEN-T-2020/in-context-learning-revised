MLP number of parameters: 20,049,921
Transformer number of parameters: 19,191,809
MLP model:MLPModel(
  (_backbone): MLP(
    (layers): ModuleList(
      (0): Linear(in_features=65, out_features=1024, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (4): ReLU()
      (5): Linear(in_features=1024, out_features=1024, bias=True)
      (6): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
      (8): Linear(in_features=1024, out_features=1024, bias=True)
      (9): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (10): ReLU()
      (11): Linear(in_features=1024, out_features=1024, bias=True)
      (12): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (13): ReLU()
      (14): Linear(in_features=1024, out_features=1024, bias=True)
      (15): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (16): ReLU()
      (17): Linear(in_features=1024, out_features=1024, bias=True)
      (18): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (19): ReLU()
      (20): Linear(in_features=1024, out_features=1024, bias=True)
      (21): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (22): ReLU()
      (23): Linear(in_features=1024, out_features=1024, bias=True)
      (24): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (25): ReLU()
      (26): Linear(in_features=1024, out_features=1024, bias=True)
      (27): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (28): ReLU()
      (29): Linear(in_features=1024, out_features=1024, bias=True)
      (30): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (31): ReLU()
      (32): Linear(in_features=1024, out_features=1024, bias=True)
      (33): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (34): ReLU()
      (35): Linear(in_features=1024, out_features=1024, bias=True)
      (36): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (37): ReLU()
      (38): Linear(in_features=1024, out_features=1024, bias=True)
      (39): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (40): ReLU()
      (41): Linear(in_features=1024, out_features=1024, bias=True)
      (42): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (43): ReLU()
      (44): Linear(in_features=1024, out_features=1024, bias=True)
      (45): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (46): ReLU()
      (47): Linear(in_features=1024, out_features=1024, bias=True)
      (48): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (49): ReLU()
      (50): Linear(in_features=1024, out_features=1024, bias=True)
      (51): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (52): ReLU()
      (53): Linear(in_features=1024, out_features=1024, bias=True)
      (54): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (55): ReLU()
      (56): Linear(in_features=1024, out_features=1024, bias=True)
      (57): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (58): ReLU()
      (59): Linear(in_features=1024, out_features=1, bias=True)
    )
  )
)
Transformer model:TransformerModel(
  (_read_in): Linear(in_features=5, out_features=256, bias=True)
  (_backbone): GPT2Model(
    (wte): Embedding(50257, 256)
    (wpe): Embedding(22, 256)
    (drop): Dropout(p=0.0, inplace=False)
    (h): ModuleList(
      (0): GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.0, inplace=False)
          (resid_dropout): Dropout(p=0.0, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (1): GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.0, inplace=False)
          (resid_dropout): Dropout(p=0.0, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (2): GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.0, inplace=False)
          (resid_dropout): Dropout(p=0.0, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (3): GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.0, inplace=False)
          (resid_dropout): Dropout(p=0.0, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (4): GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.0, inplace=False)
          (resid_dropout): Dropout(p=0.0, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (5): GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.0, inplace=False)
          (resid_dropout): Dropout(p=0.0, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (6): GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.0, inplace=False)
          (resid_dropout): Dropout(p=0.0, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (7): GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.0, inplace=False)
          (resid_dropout): Dropout(p=0.0, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (_read_out): Linear(in_features=256, out_features=1, bias=True)
)
