model:
    family: lstm
    n_embd: 256
    n_layer: 10
    n_head: 8 # do not need if model is mlp or lstm
    n_dims: 20
    n_positions: 101
    p_dropout: 0
    has_p_embedding: False

training:
    # resume_id: gpt_dim10_l12 # use for run_id and in the folder name

    task: linear_regression
    data: gaussian
    task_kwargs: {}
    batch_size: 64 #originally 64
    learning_rate: 0.0001 #originally 0.0001
    save_every_steps: 1000 #originally 1000
    keep_every_steps: 100000
    train_steps: 5001 
    # num_training_examples: 100 # total number of training examples
    curriculum:
        dims:
            start: 20
            end: 20
            inc: 1
            interval: 2000
        points:
            start: 41
            end: 41
            inc: 2
            interval: 2000

out_dir: ../models/test

wandb:
    name: "test"
    project: test
    entity: tianqi_chen
    notes:
    log_every_steps: 100
