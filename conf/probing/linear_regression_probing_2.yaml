model:
  family: lstm
  has_p_embedding: false
  n_dims: 20
  n_embd: 512
  n_head: 8
  n_layer: 5
  n_positions: 101
  p_dropout: 0
  use_first_n_layer: 2
out_dir: ../models/by_layer_linear/
training:
  batch_size: 64
  curriculum:
    dims:
      end: 20
      inc: 1
      interval: 2000
      start: 20
    points:
      end: 41
      inc: 2
      interval: 2000
      start: 41
  data: gaussian
  keep_every_steps: 100000
  learning_rate: 0.0001
  resume_id: probing_2
  save_every_steps: 1000
  task: linear_regression
  task_kwargs: {}
  train_steps: 10001
wandb:
  entity: tianqi_chen
  log_every_steps: 100
  name: linear_regression_probing_lstm_2
  notes: null
  project: in-context-training-probtest
